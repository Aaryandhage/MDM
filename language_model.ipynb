{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a566072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sentence: this is a test\n",
      "Perplexity: 3.7327440761235615\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "# Sample corpus\n",
    "text = [\n",
    "    [\"this\", \"is\", \"a\", \"test\"],\n",
    "    [\"this\", \"is\", \"another\", \"test\"],\n",
    "    [\"we\", \"are\", \"testing\", \"language\", \"model\"]\n",
    "]\n",
    "\n",
    "# Step 1: Preprocess data for bigram model\n",
    "n = 2  # Bigram model\n",
    "train_data, vocab = padded_everygram_pipeline(n, text)\n",
    "\n",
    "# Step 2: Train the MLE model\n",
    "model = MLE(n)\n",
    "model.fit(train_data, vocab)\n",
    "\n",
    "# Step 3: Test sentence\n",
    "test_sentence = [\"this\", \"is\", \"a\", \"test\"]\n",
    "\n",
    "# Step 4: Compute perplexity\n",
    "test_data, _ = padded_everygram_pipeline(n, [test_sentence])\n",
    "ppl = model.perplexity(list(next(test_data)))\n",
    "\n",
    "print(\"Test Sentence:\", \" \".join(test_sentence))\n",
    "print(\"Perplexity:\", ppl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329feb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
